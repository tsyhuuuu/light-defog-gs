<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Selection Tutorial - Fog Gaussian Classification</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            text-align: center;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .header h1 {
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }
        
        .header p {
            margin: 10px 0 0 0;
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .section {
            background: white;
            margin: 20px 0;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .section h2 {
            color: #667eea;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            margin-top: 0;
        }
        
        .section h3 {
            color: #764ba2;
            margin-top: 25px;
        }
        
        .method-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
            margin: 15px 0;
            transition: all 0.3s ease;
        }
        
        .method-card:hover {
            border-color: #667eea;
            box-shadow: 0 4px 8px rgba(102, 126, 234, 0.1);
        }
        
        .method-card h4 {
            color: #667eea;
            margin-top: 0;
            font-size: 1.3em;
        }
        
        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', Courier, monospace;
            position: relative;
        }
        
        .code-block::before {
            content: attr(data-lang);
            position: absolute;
            top: 5px;
            right: 10px;
            color: #a0aec0;
            font-size: 0.8em;
        }
        
        .yaml-block {
            background: #1a365d;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', Courier, monospace;
        }
        
        .highlight {
            background: linear-gradient(120deg, #ffecd2 0%, #fcb69f 100%);
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: bold;
        }
        
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 15px 0;
        }
        
        .pros, .cons {
            padding: 15px;
            border-radius: 8px;
        }
        
        .pros {
            background: #d4edda;
            border-left: 4px solid #28a745;
        }
        
        .cons {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
        }
        
        .pros h5, .cons h5 {
            margin-top: 0;
            color: #333;
        }
        
        .formula {
            background: #e3f2fd;
            border: 1px solid #2196f3;
            border-radius: 6px;
            padding: 10px;
            margin: 10px 0;
            font-family: 'Times New Roman', serif;
            text-align: center;
            font-style: italic;
        }
        
        .warning {
            background: #fff3cd;
            border: 1px solid #ffc107;
            border-radius: 6px;
            padding: 15px;
            margin: 15px 0;
        }
        
        .warning::before {
            content: "‚ö†Ô∏è ";
            font-weight: bold;
        }
        
        .tip {
            background: #d1ecf1;
            border: 1px solid #17a2b8;
            border-radius: 6px;
            padding: 15px;
            margin: 15px 0;
        }
        
        .tip::before {
            content: "üí° ";
            font-weight: bold;
        }
        
        .flow-diagram {
            background: white;
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }
        
        .flow-step {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 10px 15px;
            border-radius: 20px;
            margin: 5px;
            font-size: 0.9em;
        }
        
        .arrow {
            color: #667eea;
            font-size: 1.5em;
            margin: 0 10px;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        .comparison-table th,
        .comparison-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        .comparison-table th {
            background-color: #667eea;
            color: white;
        }
        
        .comparison-table tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .example-output {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        
        .toc {
            background: #e9ecef;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .toc h3 {
            margin-top: 0;
            color: #495057;
        }
        
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        
        .toc li {
            margin: 8px 0;
        }
        
        .toc a {
            color: #667eea;
            text-decoration: none;
            padding: 5px 10px;
            display: block;
            border-radius: 4px;
            transition: background 0.3s;
        }
        
        .toc a:hover {
            background: #667eea;
            color: white;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>üéØ Feature Selection Tutorial</h1>
        <p>Comprehensive Guide to Feature Selection in Fog Gaussian Classification</p>
    </div>

    <div class="section">
        <div class="toc">
            <h3>üìö Table of Contents</h3>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#modes">Feature Selection Modes</a></li>
                <li><a href="#statistical">Statistical Methods Explained</a></li>
                <li><a href="#categories">Feature Categories</a></li>
                <li><a href="#process">Selection Process Flow</a></li>
                <li><a href="#examples">Configuration Examples</a></li>
                <li><a href="#comparison">Method Comparison</a></li>
                <li><a href="#practical">Practical Recommendations</a></li>
            </ul>
        </div>
    </div>

    <div class="section" id="overview">
        <h2>üîç Overview</h2>
        <p>Feature selection is crucial for fog Gaussian classification as it:</p>
        <ul>
            <li><strong>Improves model performance</strong> by removing irrelevant features</li>
            <li><strong>Reduces overfitting</strong> by limiting model complexity</li>
            <li><strong>Speeds up training</strong> by working with fewer dimensions</li>
            <li><strong>Enhances generalization</strong> across different datasets</li>
            <li><strong>Provides insights</strong> into which 3D Gaussian properties matter most</li>
        </ul>

        <div class="tip">
            <strong>Key Insight:</strong> Not all features in your CSV are equally important for distinguishing fog from non-fog Gaussians. Smart feature selection can significantly improve your classification accuracy!
        </div>
    </div>

    <div class="section" id="modes">
        <h2>üéõÔ∏è Feature Selection Modes</h2>
        
        <div class="method-card">
            <h4>1. Auto Mode (Default)</h4>
            <div class="code-block" data-lang="python">
def _auto_feature_selection(self, df: pd.DataFrame, all_features: List[str]) -> List[str]:
    default_excludes = ['pos_x', 'pos_y', 'pos_z', 'rot_0', 'rot_1', 'rot_2', 'rot_3']
    selected = [feat for feat in all_features if feat not in default_excludes]
    return selected
            </div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h5>‚úÖ Pros</h5>
                    <ul>
                        <li>No configuration needed</li>
                        <li>Good default for most cases</li>
                        <li>Excludes problematic features</li>
                    </ul>
                </div>
                <div class="cons">
                    <h5>‚ùå Cons</h5>
                    <ul>
                        <li>May include irrelevant features</li>
                        <li>Not optimized for your data</li>
                        <li>Limited control</li>
                    </ul>
                </div>
            </div>

            <p><strong>How it works:</strong> Simple exclusion approach that removes position coordinates (for better generalization across scenes) and rotation parameters (often noisy), while keeping all other features.</p>
        </div>

        <div class="method-card">
            <h4>2. Manual Mode</h4>
            <div class="yaml-block">
features:
  selection_mode: "manual"
  manual_features: [
    "f_dc_0", "f_dc_1", "f_dc_2",      # Spherical harmonics (color)
    "scale_x", "scale_y", "scale_z",    # Scale parameters
    "opacity",                          # Opacity
    "knn_6_density",                    # Neighborhood density
    "local_linearity"                   # Geometric property
  ]
            </div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h5>‚úÖ Pros</h5>
                    <ul>
                        <li>Complete control</li>
                        <li>Use domain knowledge</li>
                        <li>Reproducible experiments</li>
                    </ul>
                </div>
                <div class="cons">
                    <h5>‚ùå Cons</h5>
                    <ul>
                        <li>Requires feature knowledge</li>
                        <li>May miss important features</li>
                        <li>Time-consuming to optimize</li>
                    </ul>
                </div>
            </div>

            <p><strong>How it works:</strong> You specify exactly which features to use in the YAML configuration. The system validates they exist and uses only those features.</p>
        </div>

        <div class="method-card">
            <h4>3. Categories Mode</h4>
            <div class="yaml-block">
features:
  selection_mode: "categories"
  use_categories: ["basic", "neighborhood", "geometric"]
  
  feature_categories:
    basic: ["f_dc_0", "f_dc_1", "f_dc_2", "scale_x", "scale_y", "scale_z", "opacity"]
    neighborhood: ["knn_6_mean_distance", "knn_6_density", "radius_0.2_count"]
    geometric: ["local_linearity", "local_planarity", "local_sphericity"]
            </div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h5>‚úÖ Pros</h5>
                    <ul>
                        <li>Easy to experiment</li>
                        <li>Semantic grouping</li>
                        <li>Mix and match types</li>
                    </ul>
                </div>
                <div class="cons">
                    <h5>‚ùå Cons</h5>
                    <ul>
                        <li>Limited to predefined groups</li>
                        <li>May include irrelevant features within categories</li>
                    </ul>
                </div>
            </div>

            <p><strong>How it works:</strong> Features are grouped by semantic meaning. You select which groups to include, and the system combines all features from those categories.</p>
        </div>

        <div class="method-card">
            <h4>4. Exclude Mode</h4>
            <div class="yaml-block">
features:
  selection_mode: "exclude"
  exclude_features: [
    "pos_x", "pos_y", "pos_z",         # Position coordinates
    "rot_0", "rot_1", "rot_2", "rot_3", # Rotation parameters
    "beta", "alpha", "id"               # Metadata
  ]
            </div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h5>‚úÖ Pros</h5>
                    <ul>
                        <li>Keep most features</li>
                        <li>Easy to remove problematic ones</li>
                        <li>Good when you know what to avoid</li>
                    </ul>
                </div>
                <div class="cons">
                    <h5>‚ùå Cons</h5>
                    <ul>
                        <li>May include many irrelevant features</li>
                        <li>Not optimized for your specific task</li>
                    </ul>
                </div>
            </div>

            <p><strong>How it works:</strong> Start with all available features, then remove the ones you specify. Good when you know which features cause problems but want to keep most others.</p>
        </div>
    </div>

    <div class="section" id="statistical">
        <h2>üìä Statistical Methods Explained</h2>
        
        <div class="method-card">
            <h4>A. Mutual Information (Recommended)</h4>
            <div class="code-block" data-lang="python">
selector = SelectKBest(score_func=mutual_info_classif, k=min(k_best, len(all_features)))
X_selected = selector.fit_transform(X, y)
selected_features = [all_features[i] for i in selector.get_support(indices=True)]
            </div>
            
            <div class="formula">
                MI(X,Y) = ‚àë‚àë p(x,y) √ó log(p(x,y) / (p(x) √ó p(y)))
            </div>
            
            <p><strong>How it works:</strong></p>
            <ul>
                <li>Measures <span class="highlight">dependency</span> between each feature and the target (is_fog)</li>
                <li>Calculates how much information about fog/no-fog is gained by knowing the feature value</li>
                <li>Higher mutual information = more informative feature</li>
                <li>Works well for both <strong>linear and non-linear relationships</strong></li>
            </ul>
            
            <div class="example-output">
Feature selection mode: statistical
Using statistical feature selection: mutual_info
Top 10 feature scores:
  knn_6_density: 0.2847
  local_linearity: 0.2156  
  opacity: 0.1923
  scale_x: 0.1467
  f_dc_0: 0.1234
  ...
Selected 15 features
            </div>
            
            <div class="tip">
                <strong>Best for:</strong> Most situations, especially when you don't know if relationships are linear or non-linear. Very robust method.
            </div>
        </div>

        <div class="method-card">
            <h4>B. Chi-Squared Test</h4>
            <div class="code-block" data-lang="python">
X_positive = X - X.min() + 1e-8  # Make features positive
selector = SelectKBest(score_func=chi2, k=min(k_best, len(all_features)))
            </div>
            
            <div class="formula">
                œá¬≤ = ‚àë (observed - expected)¬≤ / expected
            </div>
            
            <p><strong>How it works:</strong></p>
            <ul>
                <li>Tests <span class="highlight">independence</span> between each feature and the target</li>
                <li>Measures how much observed distribution differs from what we'd expect if feature and target were independent</li>
                <li>Requires non-negative features (automatically handled)</li>
                <li>Good for categorical or discrete-like features</li>
            </ul>
            
            <div class="warning">
                <strong>Limitation:</strong> Assumes features are non-negative. The code handles this by shifting values, but this may affect interpretation.
            </div>
        </div>

        <div class="method-card">
            <h4>C. F-Statistic (ANOVA)</h4>
            <div class="code-block" data-lang="python">
selector = SelectKBest(score_func=f_classif, k=min(k_best, len(all_features)))
            </div>
            
            <div class="formula">
                F = (variance between groups) / (variance within groups)
            </div>
            
            <p><strong>How it works:</strong></p>
            <ul>
                <li>ANOVA F-test for classification</li>
                <li>Measures <span class="highlight">linear relationship</span> between features and target</li>
                <li>Compares variance between fog/no-fog groups vs. variance within groups</li>
                <li>Higher F-score = better separation between classes</li>
            </ul>
            
            <div class="tip">
                <strong>Best for:</strong> When you expect mostly linear relationships between features and the fog/no-fog classification.
            </div>
        </div>

        <div class="method-card">
            <h4>D. Random Forest Importance</h4>
            <div class="code-block" data-lang="python">
rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X, y)
importances = rf.feature_importances_
            </div>
            
            <p><strong>How it works:</strong></p>
            <ul>
                <li>Uses Random Forest's built-in <span class="highlight">feature importance</span> calculation</li>
                <li>Measures how much each feature decreases impurity (Gini) when used for splits</li>
                <li>Considers feature interactions and non-linear relationships</li>
                <li>Robust to outliers and handles mixed data types well</li>
            </ul>
            
            <div class="tip">
                <strong>Best for:</strong> Complex datasets with feature interactions. Very robust and interpretable method.
            </div>
        </div>

        <div class="method-card">
            <h4>E. Variance Threshold</h4>
            <div class="code-block" data-lang="python">
selector = VarianceThreshold(threshold=threshold)
X_selected = selector.fit_transform(X)
# Then select top k by remaining variance
            </div>
            
            <p><strong>How it works:</strong></p>
            <ul>
                <li>Removes features with <span class="highlight">low variance</span> (nearly constant values)</li>
                <li>Features with little variation can't be informative for classification</li>
                <li>Then selects top k features by remaining variance</li>
                <li>Good preprocessing step to remove uninformative features</li>
            </ul>
            
            <div class="warning">
                <strong>Note:</strong> This method doesn't consider the relationship with the target, only feature variability.
            </div>
        </div>
    </div>

    <div class="section" id="categories">
        <h2>üè∑Ô∏è Feature Categories</h2>
        
        <table class="comparison-table">
            <tr>
                <th>Category</th>
                <th>Description</th>
                <th>Example Features</th>
                <th>Use Case</th>
            </tr>
            <tr>
                <td><strong>Basic</strong></td>
                <td>Core 3D Gaussian properties</td>
                <td>f_dc_0, f_dc_1, f_dc_2, scale_x, scale_y, scale_z, opacity</td>
                <td>Essential features, always include</td>
            </tr>
            <tr>
                <td><strong>Spatial</strong></td>
                <td>Position and location features</td>
                <td>pos_x, pos_y, pos_z, distance_from_center, height_percentile</td>
                <td>Scene-specific analysis, exclude for generalization</td>
            </tr>
            <tr>
                <td><strong>Rotation</strong></td>
                <td>Orientation parameters</td>
                <td>rot_0, rot_1, rot_2, rot_3</td>
                <td>Often noisy, usually excluded</td>
            </tr>
            <tr>
                <td><strong>Neighborhood</strong></td>
                <td>Local spatial context</td>
                <td>knn_6_density, knn_12_mean_distance, radius_0.2_count</td>
                <td>Captures local fog patterns</td>
            </tr>
            <tr>
                <td><strong>Geometric</strong></td>
                <td>Local shape descriptors</td>
                <td>local_linearity, local_planarity, local_sphericity</td>
                <td>Fog vs object shape differences</td>
            </tr>
            <tr>
                <td><strong>Context</strong></td>
                <td>Relative properties</td>
                <td>scale_x_neighborhood_ratio, scale_y_neighborhood_ratio</td>
                <td>How different from surrounding Gaussians</td>
            </tr>
        </table>
        
        <div class="tip">
            <strong>Recommendation:</strong> Start with ["basic", "neighborhood", "geometric"] for most fog classification tasks.
        </div>
    </div>

    <div class="section" id="process">
        <h2>üîÑ Feature Selection Process Flow</h2>
        
        <div class="flow-diagram">
            <div class="flow-step">Load CSV Data</div>
            <span class="arrow">‚Üí</span>
            <div class="flow-step">Remove Target & Metadata</div>
            <span class="arrow">‚Üí</span>
            <div class="flow-step">Apply Selection Method</div>
            <span class="arrow">‚Üí</span>
            <div class="flow-step">Validate Features Exist</div>
            <span class="arrow">‚Üí</span>
            <div class="flow-step">Calculate Importance Scores</div>
            <span class="arrow">‚Üí</span>
            <div class="flow-step">Save Selection Results</div>
        </div>

        <div class="code-block" data-lang="python">
# Detailed process implementation
def select_features(self, df: pd.DataFrame, target_column: str = 'is_fog') -> List[str]:
    # 1. Get all available features (excluding target and metadata)
    all_features = self._get_all_available_features(df)
    
    # 2. Apply the configured selection method
    if selection_mode == 'statistical':
        selected_features = self._statistical_feature_selection(df, config, target_column)
    elif selection_mode == 'manual':
        selected_features = self._manual_feature_selection(df, config)
    # ... other methods
    
    # 3. Validate selected features exist in the dataframe
    selected_features = self._validate_features(df, selected_features)
    
    # 4. Log results and save importance scores
    self.logger.info(f"Selected {len(selected_features)} features")
    
    return selected_features
        </div>
    </div>

    <div class="section" id="examples">
        <h2>‚öôÔ∏è Configuration Examples</h2>
        
        <div class="method-card">
            <h4>Example 1: Basic Features Only</h4>
            <div class="yaml-block">
# File: config/experiment_config_basic_features.yaml
dataset:
  features:
    selection_mode: "manual"
    manual_features: [
      "f_dc_0", "f_dc_1", "f_dc_2",      # Color information
      "scale_x", "scale_y", "scale_z",    # Size information  
      "opacity"                           # Transparency
    ]
            </div>
            <p><strong>Use case:</strong> Testing if core Gaussian properties alone are sufficient for fog classification.</p>
            
            <div class="code-block" data-lang="bash">
python enhanced_run_classification.py --config config/experiment_config_basic_features.yaml
            </div>
        </div>

        <div class="method-card">
            <h4>Example 2: Neighborhood-Based Features</h4>
            <div class="yaml-block">
# File: config/experiment_config_neighborhood_features.yaml
dataset:
  features:
    selection_mode: "categories"
    use_categories: ["basic", "neighborhood", "geometric"]
            </div>
            <p><strong>Use case:</strong> Including spatial context features to capture local fog patterns.</p>
        </div>

        <div class="method-card">
            <h4>Example 3: Statistical Selection</h4>
            <div class="yaml-block">
# File: config/experiment_config_statistical_selection.yaml
dataset:
  features:
    selection_mode: "statistical"
    statistical_selection:
      method: "mutual_info"
      k_best: 15
      threshold: 0.01
            </div>
            <p><strong>Use case:</strong> Let the algorithm automatically find the most informative features.</p>
            
            <div class="example-output">
Expected output:
Feature selection mode: statistical
Using statistical feature selection: mutual_info
Top 10 feature scores:
  knn_6_density: 0.2847
  local_linearity: 0.2156  
  opacity: 0.1923
  scale_x: 0.1467
  f_dc_0: 0.1234
Selected 15 features: ['knn_6_density', 'local_linearity', 'opacity', ...]
            </div>
        </div>

        <div class="method-card">
            <h4>Example 4: Custom Exclusion</h4>
            <div class="yaml-block">
dataset:
  features:
    selection_mode: "exclude"
    exclude_features: [
      # Remove position for generalization
      "pos_x", "pos_y", "pos_z",
      # Remove noisy rotation parameters
      "rot_0", "rot_1", "rot_2", "rot_3",
      # Remove specific problematic features
      "knn_6_max_distance", "radius_0.6_mean_distance"
    ]
            </div>
            <p><strong>Use case:</strong> You know which features to avoid but want to keep most others.</p>
        </div>
    </div>

    <div class="section" id="comparison">
        <h2>‚öñÔ∏è Method Comparison</h2>
        
        <table class="comparison-table">
            <tr>
                <th>Method</th>
                <th>Computational Cost</th>
                <th>Handles Non-linear</th>
                <th>Feature Interactions</th>
                <th>Best Use Case</th>
            </tr>
            <tr>
                <td><strong>Mutual Information</strong></td>
                <td>Medium</td>
                <td>‚úÖ Yes</td>
                <td>‚ö†Ô∏è Limited</td>
                <td>General purpose, robust</td>
            </tr>
            <tr>
                <td><strong>Chi-Squared</strong></td>
                <td>Low</td>
                <td>‚ùå No</td>
                <td>‚ùå No</td>
                <td>Categorical-like features</td>
            </tr>
            <tr>
                <td><strong>F-Statistic</strong></td>
                <td>Low</td>
                <td>‚ùå No</td>
                <td>‚ùå No</td>
                <td>Linear relationships</td>
            </tr>
            <tr>
                <td><strong>Random Forest</strong></td>
                <td>High</td>
                <td>‚úÖ Yes</td>
                <td>‚úÖ Yes</td>
                <td>Complex interactions</td>
            </tr>
            <tr>
                <td><strong>Variance Threshold</strong></td>
                <td>Very Low</td>
                <td>N/A</td>
                <td>N/A</td>
                <td>Removing constant features</td>
            </tr>
            <tr>
                <td><strong>Manual</strong></td>
                <td>None</td>
                <td>N/A</td>
                <td>N/A</td>
                <td>Domain expertise</td>
            </tr>
        </table>
    </div>

    <div class="section" id="practical">
        <h2>üéØ Practical Recommendations</h2>
        
        <div class="method-card">
            <h4>For Beginners</h4>
            <ol>
                <li><strong>Start with Auto mode</strong> - good default performance</li>
                <li><strong>Try Categories mode</strong> with ["basic", "neighborhood"] 
                <li><strong>Experiment with Statistical mode</strong> using "mutual_info"</li>
                <li><strong>Compare results</strong> and see which works best for your data</li>
            </ol>
        </div>

        <div class="method-card">
            <h4>For Research/Analysis</h4>
            <ol>
                <li><strong>Use Statistical mode</strong> to discover important features</li>
                <li><strong>Analyze feature importance scores</strong> to understand your data</li>
                <li><strong>Use Manual mode</strong> to test hypotheses about specific features</li>
                <li><strong>Try different methods</strong> and compare results systematically</li>
            </ol>
        </div>

        <div class="method-card">
            <h4>For Production/Deployment</h4>
            <ol>
                <li><strong>Use Manual mode</strong> with features proven to work well</li>
                <li><strong>Minimize feature count</strong> for faster inference</li>
                <li><strong>Exclude position features</strong> for better generalization</li>
                <li><strong>Validate on multiple datasets</strong> before deployment</li>
            </ol>
        </div>

        <div class="tip">
            <strong>Pro Tip:</strong> Run experiments with different feature selection methods and compare their performance. The "best" method depends on your specific data and use case!
        </div>

        <div class="warning">
            <strong>Common Mistake:</strong> Including too many irrelevant features can hurt performance more than having too few relevant ones. Quality over quantity!
        </div>

        <h3>Quick Start Commands</h3>
        <div class="code-block" data-lang="bash">
# Test basic features only
python enhanced_run_classification.py --config config/experiment_config_basic_features.yaml

# Test with neighborhood features  
python enhanced_run_classification.py --config config/experiment_config_neighborhood_features.yaml

# Let algorithm choose features automatically
python enhanced_run_classification.py --config config/experiment_config_statistical_selection.yaml

# Test feature selection on your data
python feature_selector.py --data-file data/your_dataset.csv --config-file config/experiment_config.yaml
        </div>

        <h3>Interpreting Results</h3>
        <p>After running experiments, check these files:</p>
        <ul>
            <li><code>experiments/*/results/feature_selection_info.json</code> - Which features were selected</li>
            <li><code>experiments/*/results/feature_importance.csv</code> - Feature importance scores</li>
            <li><code>experiments/*/results/experiment_report.html</code> - Complete results summary</li>
        </ul>
    </div>

    <div class="section">
        <h2>üéâ Conclusion</h2>
        <p>Feature selection is a powerful tool for improving your fog Gaussian classification results. The system provides multiple approaches:</p>
        
        <ul>
            <li><strong>üéõÔ∏è Control:</strong> Manual and Categories modes for precise feature selection</li>
            <li><strong>üß† Intelligence:</strong> Statistical methods for automatic optimization</li>
            <li><strong>‚ö° Speed:</strong> Exclude mode for quick filtering</li>
            <li><strong>üîÑ Flexibility:</strong> Easy to experiment with different approaches</li>
        </ul>
        
        <div class="tip">
            <strong>Remember:</strong> The best feature selection method depends on your specific data and research questions. Experiment with different approaches and compare their performance on your fog Gaussian datasets!
        </div>
        
        <p style="text-align: center; margin-top: 30px; color: #667eea; font-size: 1.2em;">
            Happy Feature Selecting! üöÄ
        </p>
    </div>
</body>
</html>